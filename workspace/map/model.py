# -*- coding: utf-8 -*-
"""playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_c24wTpp5eOrrP-OYSeAb70GqgiXR1ID
"""

from torch import nn, rand, flatten, stack, device
import torch
from torchvision import models, io, transforms

import pytorch_lightning as pl

import numpy as np

from PIL import Image


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

resnet_weights = models.ResNet18_Weights.IMAGENET1K_V1
resnet_transforms = resnet_weights.transforms()
e_net_weights = models.EfficientNet_V2_M_Weights.IMAGENET1K_V1
e_net_transforms = e_net_weights.transforms()


class SPTM(nn.Module):
    def __init__(self) -> None:
        super(SPTM, self).__init__()
        self.encoder = models.resnet18(weights=resnet_weights)
        modules = list(self.encoder.children())[:-1]  # delete the last fc layer.
        self.encoder = nn.Sequential(*modules)
        ### Now set requires_grad to false
        for param in self.encoder.parameters():
            param.requires_grad = False

        self.linear_layers = nn.Sequential(
            nn.Linear(512 + 512, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 2),
        )
        self.softmax = nn.Softmax(dim=1)
        self.sigmoid=nn.Sigmoid()

    def flatten(self, z):
        z = nn.functional.adaptive_avg_pool2d(z, (1, 1))
        z = flatten(z, 1)
        return z

    def forward(self, inp):
        # print("shape",inp.shape)
        obs_img = inp[:, 0]
        goal_img = inp[:, 1]

        obs_img = torch.tensor(obs_img).to(device)
        goal_img = torch.tensor(goal_img).to(device)
        # obs_img=stack(obs_img).to(device)
        # goal_img=stack(goal_img).to(device)
        obs_encoding = self.encoder(obs_img)
        obs_encoding = self.flatten(obs_encoding)
        goal_encoding = self.encoder(goal_img)
        goal_encoding = self.flatten(goal_encoding)
        z = torch.cat([obs_encoding, goal_encoding], dim=1)
        # print("z",z.shape)
        # z = self.flatten(z)
        z = self.linear_layers(z)
        # z = self.softmax(z)
        z = self.sigmoid(z)
        torch.round(z)
        return z


model = SPTM()

"""##Generate data"""


def to_categorical(y, num_classes):
    """ 1-hot encodes a tensor """
    return np.eye(num_classes, dtype='uint8')[y]


def load_image_resnet(path):
    # load the image and convert into
    # numpy array
    img = Image.open(path)
    # asarray() class is used to convert
    # PIL images into NumPy arrays
    # numpydata = asarray(img)
    return resnet_transforms(img)


class SPTMModule(pl.LightningModule):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = nn.functional.cross_entropy(y_hat, torch.tensor(y).float().to(y_hat.device))
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = nn.functional.cross_entropy(y_hat, torch.tensor(y).float().to(y_hat.device))
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=25e-5)


def load_image_vanilla(path):
    # load the image and convert into
    # numpy array
    img = Image.open(path)
    return e_net_transforms(img)


#
# def generator_custom():
#     while True:
#
#         # for i in range(len(dt)):
#         x1, x2 = random.sample(dt, 2)
#         time_distance = x1[1] - x2[1]
#         if time_distance < 0:
#             y = 0
#         else:
#             y = time_distance
#         future_x = load_image_resnet(x2[0])
#         current_x = load_image_resnet(x1[0])
#
#         yield stack([current_x, future_x]), y
#
#
# # for i in zip(generator_custom()):
# #   print(i[0])

class VanillaSimilarity(nn.Module):
    def __init__(self) -> None:
        super(VanillaSimilarity, self).__init__()
        self.encoder = models.efficientnet_v2_m(weights=e_net_weights).features
        for param in self.encoder.parameters():
            param.requires_grad = False
        self.linear_layers = nn.Sequential(
            nn.Linear(self.encoder[-1].out_channels + self.encoder[-1].out_channels, 256),
            nn.ReLU(),
            nn.Linear(256, 32),
            nn.ReLU(),
        )
        self.predictor = nn.Sequential(
            nn.Linear(32, 1),
        )

    def flatten(self, z):
        z = nn.functional.adaptive_avg_pool2d(z, (1, 1))
        z = flatten(z, 1)
        return z

    def forward(self, inp):
        obs_img = inp[:, 0]
        goal_img = inp[:, 1]
        # print(goal_img.shape)
        obs_img = obs_img.to(device)
        goal_img = goal_img.to(device)
        obs_encoding = self.encoder(obs_img)
        obs_encoding = self.flatten(obs_encoding)

        goal_encoding = self.encoder(goal_img)
        goal_encoding = self.flatten(goal_encoding)

        z = torch.cat([obs_encoding, goal_encoding], dim=1)
        z = self.linear_layers(z)
        similarity = self.predictor(z)
        return similarity


class VanillaSimilarityModule(pl.LightningModule):
    def __init__(self, model):
        super().__init__()
        self.model = model
        # self.mse = MeanSquaredError()
        # self.mse=self.mse.to(device)

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        y_hat = y_hat.squeeze(1)
        # print("loss",y_hat.shape)
        loss = nn.functional.mse_loss(y_hat, torch.tensor(y).float().to(y_hat.device))
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=32)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        y_hat = y_hat.squeeze(1)
        # print("loss",y_hat.shape)
        loss = nn.functional.mse_loss(y_hat, torch.tensor(y).float().to(y_hat.device))
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=32)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-4)


# benchmark vanilla and sptm
def convert_continous_to_discrete_categorical(x):
    y = nn.functional.sigmoid(x) > 0.6
    y = y.squeeze()
    print(y.shape)
    y = nn.functional.one_hot(y, num_classes=2)
    return y


# from torchmetrics.classification  import F1Score
# f1_score = F1Score(task="multiclass",num_classes=2).to(device)
# vn=[]
# sptm=[]
# nnscs=[]
# for i in range(350):
#   validation_data=generator_sptm_validation()
#   x,y=validation_data
#   # print(x[:])
#
#   y=torch.tensor(y).float().to(device)
#   sptm_y=sptm_model(x)
#   sptm_y=torch.round(sptm_y)
#
#   vanilla_y=vanilla_model(x)*10
#   vanilla_y=(vanilla_y>0.5).long()
#
#   vanilla_y=nn.functional.one_hot(vanilla_y,num_classes=2).float().squeeze(1)
#   sptm.append(f1_score(sptm_y,y).cpu())
#   vn.append(f1_score(vanilla_y,y).cpu())

def load_sptm_base_model():
    model = SPTM()
    sptm_model = SPTMModule.load_from_checkpoint(
        "./checkpoints/sptm.ckpt", model=model)
    sptm_model.eval()
    return model


def load_enet_model():
    model = VanillaSimilarity()
    vanilla_model = VanillaSimilarityModule.load_from_checkpoint(
        "/content/drive/MyDrive/Colab Notebooks/sptm/vanilla/epoch=27-step=2812.ckpt", model=model)
    vanilla_model.eval()
    return model
